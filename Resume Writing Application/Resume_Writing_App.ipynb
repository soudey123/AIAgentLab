{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Install dependencies\n",
        "\n",
        "!pip install -q streamlit crewai crewai_tools reportlab python-docx"
      ],
      "metadata": {
        "id": "Q0i7SvV07fXU"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FPiQmEK6j1K",
        "outputId": "bb9acb8c-4cf2-4268-f530-202df2b3cf81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import os\n",
        "import streamlit as st\n",
        "from crewai import Agent, Task, Crew\n",
        "from crewai_tools import SerperDevTool, ScrapeWebsiteTool, FileReadTool, MDXSearchTool\n",
        "import io\n",
        "import docx\n",
        "from reportlab.pdfgen import canvas\n",
        "from reportlab.lib.pagesizes import letter\n",
        "import traceback\n",
        "import tempfile\n",
        "\n",
        "def create_docx(content, filename):\n",
        "    \"\"\"Create a Word document from text content\"\"\"\n",
        "    doc = docx.Document()\n",
        "    for line in content.split('\\n'):\n",
        "        doc.add_paragraph(line)\n",
        "\n",
        "    doc_io = io.BytesIO()\n",
        "    doc.save(doc_io)\n",
        "    doc_io.seek(0)\n",
        "    return doc_io\n",
        "\n",
        "def setup_crew(job_posting_url, github_url, personal_writeup, openai_api_key, serper_api_key, gpt_model, resume_file_path):\n",
        "    \"\"\"Set up CrewAI agents and tasks\"\"\"\n",
        "    # Extensive API key validation\n",
        "    if not openai_api_key:\n",
        "        raise ValueError(\"OpenAI API Key is missing. Please provide a valid API key.\")\n",
        "    if not serper_api_key:\n",
        "        raise ValueError(\"Serper API Key is missing. Please provide a valid API key.\")\n",
        "\n",
        "    try:\n",
        "        # Explicitly set API keys\n",
        "        os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
        "        os.environ[\"SERPER_API_KEY\"] = serper_api_key\n",
        "\n",
        "        # Tools setup with explicit API keys\n",
        "        search_tool = SerperDevTool()\n",
        "        scrape_tool = ScrapeWebsiteTool()\n",
        "\n",
        "        # File-related tools with careful initialization\n",
        "        try:\n",
        "            file_read_tool = FileReadTool(file_path=resume_file_path)\n",
        "            semantic_search_tool = MDXSearchTool(file_path=resume_file_path)\n",
        "\n",
        "        except Exception as tool_error:\n",
        "            st.warning(f\"Could not initialize file-related tools: {tool_error}\")\n",
        "            file_read_tool = None\n",
        "            semantic_search_tool = None\n",
        "\n",
        "        # Agents with explicit model configuration\n",
        "        researcher = Agent(\n",
        "            role=\"Tech Job Researcher\",\n",
        "            goal=\"Make sure to do amazing analysis on job posting to help job applicants\",\n",
        "            tools=[scrape_tool, search_tool],\n",
        "            verbose=True,\n",
        "            backstory=\"As a Job Researcher, your prowess in navigating and extracting critical information from job postings is unmatched.\",\n",
        "            llm_config={\n",
        "                \"temperature\": 0.7,\n",
        "                \"model_name\": gpt_model,\n",
        "                \"api_key\": openai_api_key  # Explicitly pass API key\n",
        "            }\n",
        "        )\n",
        "\n",
        "        profiler = Agent(\n",
        "            role=\"Personal Profiler for Engineers\",\n",
        "            goal=\"Do incredible research on job applicants to help them stand out in the job market\",\n",
        "            tools=[scrape_tool, search_tool,\n",
        "                   file_read_tool, semantic_search_tool],\n",
        "            verbose=True,\n",
        "            backstory=\"Equipped with analytical prowess, you dissect and synthesize information from diverse sources to craft comprehensive personal and professional profiles.\",\n",
        "            llm_config={\n",
        "                \"temperature\": 0.7,\n",
        "                \"model_name\": gpt_model,\n",
        "                \"api_key\": openai_api_key\n",
        "            }\n",
        "        )\n",
        "\n",
        "        resume_strategist = Agent(\n",
        "            role=\"Resume Strategist for Engineers\",\n",
        "            goal=\"Find all the best ways to make a resume stand out in the job market\",\n",
        "            tools=[scrape_tool, search_tool,\n",
        "                   file_read_tool, semantic_search_tool],\n",
        "            verbose=True,\n",
        "            backstory=\"With a strategic mind and an eye for detail, you excel at refining resumes to highlight the most relevant skills and experiences.\",\n",
        "            llm_config={\n",
        "                \"temperature\": 0.7,\n",
        "                \"model_name\": gpt_model,\n",
        "                \"api_key\": openai_api_key\n",
        "            }\n",
        "        )\n",
        "\n",
        "        interview_preparer = Agent(\n",
        "            role=\"Engineering Interview Preparer\",\n",
        "            goal=\"Create interview questions and talking points based on the resume and job requirements\",\n",
        "            tools=[scrape_tool, search_tool,\n",
        "                   file_read_tool, semantic_search_tool],\n",
        "            verbose=True,\n",
        "            backstory=\"Your role is crucial in anticipating the dynamics of interviews, formulating key questions and talking points.\",\n",
        "            llm_config={\n",
        "                \"temperature\": 0.7,\n",
        "                \"model_name\": gpt_model,\n",
        "                \"api_key\": openai_api_key\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # Tasks\n",
        "        research_task = Task(\n",
        "            description=f\"Analyze the job posting URL {job_posting_url} to extract key skills, experiences, and qualifications required.\",\n",
        "            expected_output=\"A structured list of job requirements, including necessary skills, qualifications, and experiences.\",\n",
        "            agent=researcher\n",
        "        )\n",
        "\n",
        "        profile_task = Task(\n",
        "            description=f\"Compile a detailed personal and professional profile using GitHub URL {github_url} and personal write-up.\",\n",
        "            expected_output=\"A comprehensive profile document that includes skills, project experiences, contributions, interests, and communication style.\",\n",
        "            agent=profiler\n",
        "        )\n",
        "\n",
        "        resume_strategy_task = Task(\n",
        "            description=\"Using the profile and job requirements, tailor the resume to highlight the most relevant areas.\",\n",
        "            expected_output=\"An updated resume that effectively highlights the candidate's qualifications and experiences.\",\n",
        "            agent=resume_strategist,\n",
        "            output_file=\"tailored_resume.md\",\n",
        "            context=[research_task, profile_task]\n",
        "        )\n",
        "\n",
        "        interview_preparation_task = Task(\n",
        "            description=\"Create potential interview questions and talking points based on the tailored resume and job requirements.\",\n",
        "            expected_output=\"A document containing key questions and talking points for the initial interview.\",\n",
        "            agent=interview_preparer,\n",
        "            output_file=\"interview_materials.md\",\n",
        "            context=[research_task, profile_task, resume_strategy_task]\n",
        "        )\n",
        "\n",
        "        # Crew setup\n",
        "        job_application_crew = Crew(\n",
        "            agents=[researcher, profiler, resume_strategist, interview_preparer],\n",
        "            tasks=[research_task, profile_task, resume_strategy_task, interview_preparation_task],\n",
        "            verbose=True\n",
        "        )\n",
        "\n",
        "        # Inputs\n",
        "        job_application_inputs = {\n",
        "            'job_posting_url': job_posting_url,\n",
        "            'github_url': github_url,\n",
        "            'personal_writeup': personal_writeup\n",
        "        }\n",
        "\n",
        "        # Execute crew\n",
        "        result = job_application_crew.kickoff(inputs=job_application_inputs)\n",
        "\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        # More detailed error logging\n",
        "        st.error(f\"Error setting up CrewAI: {str(e)}\")\n",
        "        st.error(f\"Detailed Traceback:\\n{traceback.format_exc()}\")\n",
        "        st.error(f\"OpenAI API Key status: {bool(openai_api_key)}\")\n",
        "        st.error(f\"Serper API Key status: {bool(serper_api_key)}\")\n",
        "        st.error(f\"Selected GPT Model: {gpt_model}\")\n",
        "        raise\n",
        "\n",
        "def main():\n",
        "    st.title(\"Resume Tailoring AI Assistant ðŸ“„âœ¨\")\n",
        "\n",
        "    # Sidebar for API Key Configuration\n",
        "    st.sidebar.header(\"API Configuration\")\n",
        "    openai_api_key = st.sidebar.text_input(\"OpenAI API Key\", type=\"password\",\n",
        "        help=\"Your OpenAI API key from platform.openai.com\")\n",
        "    serper_api_key = st.sidebar.text_input(\"Serper API Key\", type=\"password\",\n",
        "        help=\"Your Serper API key from serper.dev\")\n",
        "\n",
        "    gpt_models = [\"gpt-3.5-turbo\", \"gpt-4\", \"gpt-4o\"]\n",
        "    gpt_model = st.sidebar.selectbox(\"Select GPT Model\", options=gpt_models)\n",
        "\n",
        "    st.sidebar.header(\"Job Application Details\")\n",
        "    job_posting_url = st.sidebar.text_input(\"Job Posting URL\")\n",
        "    github_url = st.sidebar.text_input(\"GitHub Profile URL\")\n",
        "    personal_writeup = st.sidebar.text_area(\"Personal Professional Summary\")\n",
        "    resume_file = st.sidebar.file_uploader(\"Upload Resume (Markdown/Text)\", type=['md', 'txt', 'pdf', 'docx'])\n",
        "\n",
        "    if st.sidebar.button(\"Generate Tailored Materials\"):\n",
        "        if not all([openai_api_key, serper_api_key, job_posting_url, github_url, personal_writeup, resume_file]):\n",
        "            st.error(\"Please fill out all required fields.\")\n",
        "            return\n",
        "\n",
        "        with st.spinner('Generating tailored resume and interview materials...'):\n",
        "            try:\n",
        "                # Create a temporary file\n",
        "                with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.md') as temp_file:\n",
        "                    resume_content = resume_file.getvalue().decode('utf-8')\n",
        "                    temp_file.write(resume_content)\n",
        "                    temp_file_path = temp_file.name\n",
        "\n",
        "                # Setup crew and generate materials\n",
        "                final_result = setup_crew(\n",
        "                    job_posting_url, github_url, personal_writeup,\n",
        "                    openai_api_key, serper_api_key, gpt_model, temp_file_path\n",
        "                )\n",
        "\n",
        "                # Load and create downloadable content\n",
        "                with open(\"./tailored_resume.md\", \"r\", encoding=\"utf-8\") as file:\n",
        "                    markdown_resume_content = file.read()\n",
        "                docx_resume = create_docx(markdown_resume_content, 'tailored_resume.docx')\n",
        "\n",
        "                with open(\"./interview_materials.md\", \"r\", encoding=\"utf-8\") as file:\n",
        "                    markdown_interview_content = file.read()\n",
        "                docx_interview = create_docx(markdown_interview_content, 'interview_materials.docx')\n",
        "\n",
        "                # Store results in session state\n",
        "                st.session_state[\"docx_resume\"] = docx_resume\n",
        "                st.session_state[\"docx_interview\"] = docx_interview\n",
        "\n",
        "                st.success(\"Materials generated successfully!\")\n",
        "            except Exception as e:\n",
        "                st.error(f\"Error occurred: {e}\")\n",
        "            finally:\n",
        "                os.unlink(temp_file_path)\n",
        "\n",
        "    # Display download buttons if content exists\n",
        "    if \"docx_resume\" in st.session_state and \"docx_interview\" in st.session_state:\n",
        "        st.subheader(\"Download Your Tailored Materials\")\n",
        "        col1, col2 = st.columns(2)\n",
        "        with col1:\n",
        "            st.download_button(\n",
        "                label=\"Download Word Resume\",\n",
        "                data=st.session_state[\"docx_resume\"],\n",
        "                file_name=\"tailored_resume.docx\",\n",
        "                mime=\"application/vnd.openxmlformats-officedocument.wordprocessingml.document\",\n",
        "                key=\"resume_download\"\n",
        "            )\n",
        "        with col2:\n",
        "            st.download_button(\n",
        "                label=\"Download Word Interview Materials\",\n",
        "                data=st.session_state[\"docx_interview\"],\n",
        "                file_name=\"interview_materials.docx\",\n",
        "                mime=\"application/vnd.openxmlformats-officedocument.wordprocessingml.document\",\n",
        "                key=\"interview_materials_download\"\n",
        "            )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Uncomment the following line when running in Colab\n",
        "    # install_dependencies()\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Kill any previous Streamlit instances\n",
        "!kill -9 $(lsof -t -i:8501)\n",
        "\n",
        "# Start a new ngrok tunnel\n",
        "#public_url = ngrok.connect(port='8501')\n",
        "#print(f\"Public URL for the app: {public_url}\")\n",
        "\n",
        "# Configure ngrok with your authtoken\n",
        "ngrok.set_auth_token(\"<YOUR TOKEN>\")  # Replace with your actual token\n",
        "\n",
        "\n",
        "public_url = ngrok.connect(8501, proto=\"http\")  # Specify proto=\"http\"\n",
        "print(f\"Public URL: {public_url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjkftj-F-k31",
        "outputId": "efd3e4ed-8809-409c-d4a2-0062c015036e"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kill: usage: kill [-s sigspec | -n signum | -sigspec] pid | jobspec ... or kill -l [sigspec]\n",
            "Public URL: NgrokTunnel: \"https://d4dc-34-168-138-203.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py &>/content/logs.txt &"
      ],
      "metadata": {
        "id": "PoQxQJ8V-qV8"
      },
      "execution_count": 103,
      "outputs": []
    }
  ]
}
