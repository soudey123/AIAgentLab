### Answer
In biomedical question answering, retrieval-augmented generation (RAG) systems are evaluated using a mix of retrieval metrics (e.g., NDCG, MRR, top‑k accuracy) and answer-quality metrics for different QA formats (e.g., Accuracy for close-ended/clinical correctness; ROUGE/ROUGE‑L for long-form or semantic fidelity), typically on biomedical benchmark datasets (e.g., NFCorpus, TREC‑COVID, MedQA, PubMedQA, BioASQ, MedMCQA) and/or expert-verified QA pairs, often by comparing against non‑RAG baselines and reporting improvements in accuracy and hallucination reduction. [6][7][8][4]

### Evidence
- MedBioRAG is evaluated “across text retrieval, close-ended QA, and long-form QA” on benchmark datasets including “NFCorpus, TREC-COVID, MedQA, PubMedQA, and BioASQ,” reporting retrieval improvements in “NDCG and MRR,” “higher accuracy” for close-ended QA, and “ROUGE scores” for long-form QA. [6]
- A RAG-based medical QA system is evaluated on “PubMedQA and MedMCQA,” showing “measurable improvements in answer accuracy compared to using LLMs alone,” and reporting a reduction of “unsupported content by approximately 60%.” [7]
- NurRAG evaluation uses “1,000 expert-verified nursing question-answer pairs,” assessing “semantic fidelity” with “ROUGE-L” and “clinical correctness” with “Accuracy,” and reports statistically significant improvements versus conventional LLMs. [8]
- An EMR-focused RAG chatbot evaluates retrieval by “top-k accuracy metrics,” using a dataset of “5,931 question-document pairs” validated by domain experts. [4]

### What’s missing / Unknowns
- A single, standardized biomedical QA evaluation protocol (the context shows multiple approaches/metrics across studies, but not a unified standard). [4][6][7][8]
- Details on how grounding/citation quality is quantitatively scored across biomedical RAG systems (beyond reported reductions in unsupported content or general statements about evidence-based outputs). [7][8]

---
### Sources
[1] Retrieval Augmented Generation: What Works and Lessons Learned. — europe_pmc | 2025 — https://pubmed.ncbi.nlm.nih.gov/40357591/
[2] Exploring the Potential of Retrieval Augmented Generation for Question Answering in Radiology: Initial Findings and Future Directions. — europe_pmc | 2025 — https://pubmed.ncbi.nlm.nih.gov/40380590/
[3] Joint Modeling of Intelligent Retrieval-Augmented Generation in LLM-Based Knowledge Fusion — europe_pmc | 2025 — 
[4] Development and Evaluation of a Retrieval-Augmented Generation-Based Electronic Medical Record Chatbot System. — europe_pmc | 2025 — https://pubmed.ncbi.nlm.nih.gov/40840929/
[5] Hybrid Retrieval-Augmented Generation for Robust Multilingual Document Question Answering — arxiv | 2025-12-14T13:57:05Z — https://arxiv.org/abs/2512.12694v1
[6] MedBioRAG: Semantic Search and Retrieval-Augmented Generation with Large Language Models for Medical and Biological QA — arxiv | 2025-12-10T15:43:25Z — https://arxiv.org/abs/2512.10996v1
[7] Optimizing Medical Question-Answering Systems: A Comparative Study of Fine-Tuned and Zero-Shot Large Language Models with RAG Framework — arxiv | 2025-12-05T16:38:47Z — https://arxiv.org/abs/2512.05863v1
[8] Nursing Retrieval-Augmented Generation: Retrieval augmented generation for nursing question answering with large language models. — europe_pmc | 2025 — https://pubmed.ncbi.nlm.nih.gov/41367595/
[9] Dual retrieving and ranking medical large language model with retrieval augmented generation. — europe_pmc | 2025 — https://pubmed.ncbi.nlm.nih.gov/40413225/
[10] Context-Aware Retrieval-Augmented Generation for Artificial Intelligence in Urology. — europe_pmc | 2025 — https://pubmed.ncbi.nlm.nih.gov/40821282/