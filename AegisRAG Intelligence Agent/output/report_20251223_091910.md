### Answer
Biomedical RAG question-answering systems are being evaluated using (1) benchmark QA datasets spanning close-ended and long-form biomedical QA, (2) retrieval-quality metrics for the evidence-fetching stage, (3) answer-quality metrics such as accuracy and ROUGE-style overlap for generated responses, and (4) healthcare/clinical-specific factuality and validity metrics (e.g., FactScore, RadGraph-F1, MED-F1). Evaluations also include domain-specific test sets (e.g., expert-verified nursing QA pairs; synthetic regulatory questions judged by clinicians) and structured benchmarks designed to test different RAG pipeline complexities (e.g., basic vs multi-vector vs graph-enhanced RAG). [2][3][4][5][6][8]

### Evidence
- MedBioRAG is evaluated across text retrieval, close-ended QA, and long-form QA using benchmarks NFCorpus, TREC-COVID, MedQA, PubMedQA, and BioASQ, reporting retrieval metrics (NDCG, MRR), close-ended QA accuracy, and long-form QA ROUGE. [3]
- A RAG-based medical QA system is evaluated on PubMedQA and MedMCQA and reports improved answer accuracy vs LLM-only baselines; it also reports reduction of unsupported content and provides source references for transparency. [6]
- NurRAG is evaluated with 1,000 expert-verified nursing question-answer pairs, using ROUGE-L for “semantic fidelity” and Accuracy for “clinical correctness,” comparing against conventional LLMs. [8]
- Raven (a RAG system for laboratory regulatory QA) is tested on 103 synthetic questions, with correctness/completeness judged against a board-certified pathologist’s answers; performance is analyzed separately for questions addressed vs not addressed in the authoritative CFR source. [5]
- RAGCare-QA provides 420 theoretical medical knowledge questions as an assessment dataset for RAG pipelines, labeling questions by the “best fit” RAG complexity level (Basic, Multi-vector, Graph-enhanced) across six specialties and three difficulty levels. [2]
- A healthcare RAG review reports that evaluation strategies include standard metrics and clinical-specific metrics—FactScore, RadGraph-F1, and MED-F1—aimed at factual accuracy, medical validity, and clinical relevance. [4]

### What’s missing / Unknowns
- Insufficient evidence on a single standardized evaluation protocol used universally across biomedical RAG QA (e.g., a community-wide agreed suite of datasets/metrics and how they are combined). [2][3][4][5][6][8]
- Insufficient evidence on how often human expert review is used versus automatic metrics across biomedical RAG QA studies overall, beyond the specific examples described. [5][8]

---
### Sources
[1] Exploring the Potential of Retrieval Augmented Generation for Question Answering in Radiology: Initial Findings and Future Directions. — europe_pmc | 2025 — https://pubmed.ncbi.nlm.nih.gov/40380590/
[2] RAGCare-QA: A benchmark dataset for evaluating retrieval-augmented generation pipelines in theoretical medical knowledge. — europe_pmc | 2025 — https://pubmed.ncbi.nlm.nih.gov/41140864/
[3] MedBioRAG: Semantic Search and Retrieval-Augmented Generation with Large Language Models for Medical and Biological QA — arxiv | 2025-12-10T15:43:25Z — https://arxiv.org/abs/2512.10996v1
[4] Retrieval-Augmented Generation (RAG) in Healthcare: A Comprehensive Review — europe_pmc | 2025 — 
[5] Retrieval-augmented generation for interpreting clinical laboratory regulations using large language models. — europe_pmc | 2025 — https://pubmed.ncbi.nlm.nih.gov/41244595/
[6] Optimizing Medical Question-Answering Systems: A Comparative Study of Fine-Tuned and Zero-Shot Large Language Models with RAG Framework — arxiv | 2025-12-05T16:38:47Z — https://arxiv.org/abs/2512.05863v1
[7] Retrieval-Augmented Generation in Biomedicine: A Survey of Technologies, Datasets, and Clinical Applications — europe_pmc | 2025 — 
[8] Nursing Retrieval-Augmented Generation: Retrieval augmented generation for nursing question answering with large language models. — europe_pmc | 2025 — https://pubmed.ncbi.nlm.nih.gov/41367595/