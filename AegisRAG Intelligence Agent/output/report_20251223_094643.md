### Answer
RAG systems for biomedical question answering are evaluated using benchmark QA datasets and retrieval benchmarks, measuring both retrieval quality (e.g., NDCG, MRR) and answer quality with task-appropriate metrics (e.g., accuracy for close-ended QA and ROUGE for long-form QA), alongside healthcare-focused factual/clinical validity metrics such as FactScore, RadGraph-F1, and MED-F1. [3], [4] They are also assessed with domain-specific evaluation sets (e.g., theoretical medical knowledge MCQs in multiple specialties) designed to test different RAG pipeline complexities (basic, multi-vector, graph-enhanced). [1]

### Evidence
- MedBioRAG is evaluated on text retrieval, close-ended QA, and long-form QA using benchmarks including NFCorpus, TREC-COVID, MedQA, PubMedQA, and BioASQ. [3]
- In that evaluation, retrieval quality is measured with NDCG and MRR, close-ended QA with accuracy, and long-form QA with ROUGE. [3]
- A healthcare RAG review reports evaluation strategies using both standard metrics and clinical-specific metrics, explicitly naming FactScore, RadGraph-F1, and MED-F1 as important for factual accuracy, medical validity, and clinical relevance. [4]
- RAGCare-QA provides 420 theoretical medical knowledge questions for assessing RAG pipelines across specialties and complexity levels, and tags questions by the “best fit” RAG implementation complexity (Basic RAG, Multi-vector RAG, Graph-enhanced RAG). [1]

### What’s missing / Unknowns
- Insufficient evidence on a single, standardized evaluation protocol specific to “biomedical QA” across studies (e.g., exact human evaluation rubrics, error taxonomies, or how clinical-specific metrics are applied per QA format). [4]
- Insufficient evidence on evaluation details for radiology QA beyond a “preliminary experiment” and “moderate improvements,” since specific metrics are not provided in the context. [2]

---
### Sources
[1] RAGCare-QA: A benchmark dataset for evaluating retrieval-augmented generation pipelines in theoretical medical knowledge. — europe_pmc | 2025 — https://pubmed.ncbi.nlm.nih.gov/41140864/
[2] Exploring the Potential of Retrieval Augmented Generation for Question Answering in Radiology: Initial Findings and Future Directions. — europe_pmc | 2025 — https://pubmed.ncbi.nlm.nih.gov/40380590/
[3] MedBioRAG: Semantic Search and Retrieval-Augmented Generation with Large Language Models for Medical and Biological QA — arxiv | 2025-12-10T15:43:25Z — https://arxiv.org/abs/2512.10996v1
[4] Retrieval-Augmented Generation (RAG) in Healthcare: A Comprehensive Review — europe_pmc | 2025 — 
[5] Retrieval-augmented generation for interpreting clinical laboratory regulations using large language models. — europe_pmc | 2025 — https://pubmed.ncbi.nlm.nih.gov/41244595/
[6] Retrieval-Augmented Generation: Advancing personalized care and research in oncology. — europe_pmc | 2025 — https://pubmed.ncbi.nlm.nih.gov/40068371/
[7] Retrieval-Augmented Generation in Biomedicine: A Survey of Technologies, Datasets, and Clinical Applications — europe_pmc | 2025 — 
[8] Retrieval Augmented Generation System for Mental Health Information. — europe_pmc | 2025 — https://pubmed.ncbi.nlm.nih.gov/40775947/