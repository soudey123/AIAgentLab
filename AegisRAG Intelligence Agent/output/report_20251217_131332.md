### Answer
RAG systems are evaluated using multi-dimensional suites and task/domain-specific benchmarks. Metrics reported in the provided context include factual accuracy and hallucination avoidance, as well as adversarial robustness, bias/fairness, toxicity, security, and calibration; healthcare-focused evaluations also use FactScore, RadGraph-F1, and MED-F1. Benchmarks/datasets mentioned include RAGCare-QA (medical QA) and a synthetic question set for regulatory QA (Raven), and some studies also evaluate retrieval diversity and expert-rated output quality. [1][2][3][4][10]

### Evidence
- Secure-RAG is an evaluation suite that assesses RAG systems across “factual accuracy, hallucination avoidance, adversarial robustness, bias and fairness, toxicity, security, and calibration,” using “standardized metrics” and monitoring at the query/retrieval/generation stages. [1]
- A healthcare RAG review reports evaluation using “FactScore, RadGraph-F1, and MED-F1” as clinical-specific metrics important for “factual accuracy, medical validity, and clinical relevance.” [4]
- Raven (a regulatory RAG system) was evaluated on “103 synthetic… questions”; performance was judged against a pathologist reference with outcomes like “totally complete and correct in 92.0%” and noted “little irrelevant content” and “low potential for… error.” [2]
- RAGCare-QA is described as a “benchmark dataset… of 420… medical knowledge questions for assessing RAG pipelines.” [3]
- A biomimicry RAG framework evaluation included “expert evaluations of text and design concept quality” and a review of “retrieval diversity.” [10]

### What’s missing / Unknowns
- A comprehensive list of *standard* RAG benchmarks and retrieval/generation metrics (e.g., specific retrieval precision/recall-type measures) is not provided in the context.  
- Definitions and calculation procedures for the listed metrics (e.g., how Secure-RAG computes each standardized metric) are not detailed in the context. [1]

---
### Sources
[1] Building a Security and Reliability Evaluation Suite for Retrieval-Augmented Generation (RAG) Systems — europe_pmc | 2025 — 
[2] Retrieval-augmented generation for interpreting clinical laboratory regulations using large language models. — europe_pmc | 2025 — https://pubmed.ncbi.nlm.nih.gov/41244595/
[3] RAGCare-QA: A benchmark dataset for evaluating retrieval-augmented generation pipelines in theoretical medical knowledge. — europe_pmc | 2025 — https://pubmed.ncbi.nlm.nih.gov/41140864/
[4] Retrieval-Augmented Generation (RAG) in Healthcare: A Comprehensive Review — europe_pmc | 2025 — 
[5] Retrieval-Augmented Generation in Biomedicine: A Survey of Technologies, Datasets, and Clinical Applications — europe_pmc | 2025 — 
[6] Retrieval Augmented Generation System for Mental Health Information. — europe_pmc | 2025 — https://pubmed.ncbi.nlm.nih.gov/40775947/
[7] Retrieval-Augmented Generation: Advancing personalized care and research in oncology. — europe_pmc | 2025 — https://pubmed.ncbi.nlm.nih.gov/40068371/
[8] Retrieval augmented generation for large language models in healthcare: A systematic review. — europe_pmc | 2025 — https://pubmed.ncbi.nlm.nih.gov/40498738/
[9] Retrieval Augmented Generation (RAG) and Large Language Models (LLMs) for Enterprise Knowledge Management and Document Automation: A Systematic Literature Review — europe_pmc | 2025 — 
[10] An Innovative Retrieval-Augmented Generation Framework for Stage-Specific Knowledge Translation in Biomimicry Design. — europe_pmc | 2025 — https://pubmed.ncbi.nlm.nih.gov/41002860/