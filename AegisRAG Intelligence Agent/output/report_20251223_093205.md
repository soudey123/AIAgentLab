### Answer
Biomedical RAG-based QA systems are being evaluated using (1) benchmark datasets spanning retrieval, close-ended QA, and long-form QA tasks with task-specific metrics (e.g., NDCG/MRR for retrieval; accuracy for close-ended QA; ROUGE for long-form QA) [3]; (2) domain-specific QA benchmarks such as PubMedQA and MedMCQA where retrieval augmentation is assessed via answer accuracy improvements and reductions in unsupported content [6]; (3) expert-verified clinical QA pairs with metrics like ROUGE-L for semantic fidelity and Accuracy for clinical correctness (e.g., nursing QA) [8]; (4) curated evaluation datasets specifically designed to assess RAG pipeline variants across medical specialties and complexity levels (e.g., RAGCare-QA) [2]; and (5) reviews comparing standard and clinical-specific evaluation metrics emphasizing factual accuracy/medical validity (e.g., FactScore, RadGraph-F1, MED-F1) [4].

### Evidence
- MedBioRAG is evaluated across text retrieval, close-ended QA, and long-form QA using benchmarks (NFCorpus, TREC-COVID, MedQA, PubMedQA, BioASQ) and reports improved NDCG/MRR (retrieval), higher accuracy (close-ended QA), and higher ROUGE (long-form QA) [3].
- A RAG-based medical QA system is evaluated on PubMedQA and MedMCQA and shows measurable improvements in answer accuracy versus LLM-only baselines; it also reports that grounding reduces unsupported content by ~60% and provides source references for transparency [6].
- NurRAG is evaluated with 1,000 expert-verified nursing QA pairs; semantic fidelity is measured with ROUGE-L and clinical correctness with Accuracy, showing significant improvements over conventional LLMs [8].
- RAGCare-QA provides 420 theoretical medical knowledge questions across six specialties and three complexity levels and labels questions by “best fit” RAG implementation complexity (Basic, Multi-vector, Graph-enhanced) to assess RAG pipelines in evaluation settings [2].
- A healthcare RAG review compares evaluation strategies using standard metrics and clinical-specific metrics, explicitly naming FactScore, RadGraph-F1, and MED-F1 as important for factual accuracy, medical validity, and clinical relevance [4].

### What’s missing / Unknowns
- Insufficient evidence on which specific standard metrics (beyond those explicitly named) are most commonly used across biomedical RAG QA studies, and how evaluations are standardized across datasets and specialties [2][3][4][6][8].

---
### Sources
[1] Exploring the Potential of Retrieval Augmented Generation for Question Answering in Radiology: Initial Findings and Future Directions. — europe_pmc | 2025 — https://pubmed.ncbi.nlm.nih.gov/40380590/
[2] RAGCare-QA: A benchmark dataset for evaluating retrieval-augmented generation pipelines in theoretical medical knowledge. — europe_pmc | 2025 — https://pubmed.ncbi.nlm.nih.gov/41140864/
[3] MedBioRAG: Semantic Search and Retrieval-Augmented Generation with Large Language Models for Medical and Biological QA — arxiv | 2025-12-10T15:43:25Z — https://arxiv.org/abs/2512.10996v1
[4] Retrieval-Augmented Generation (RAG) in Healthcare: A Comprehensive Review — europe_pmc | 2025 — 
[5] Retrieval-augmented generation for interpreting clinical laboratory regulations using large language models. — europe_pmc | 2025 — https://pubmed.ncbi.nlm.nih.gov/41244595/
[6] Optimizing Medical Question-Answering Systems: A Comparative Study of Fine-Tuned and Zero-Shot Large Language Models with RAG Framework — arxiv | 2025-12-05T16:38:47Z — https://arxiv.org/abs/2512.05863v1
[7] Retrieval-Augmented Generation in Biomedicine: A Survey of Technologies, Datasets, and Clinical Applications — europe_pmc | 2025 — 
[8] Nursing Retrieval-Augmented Generation: Retrieval augmented generation for nursing question answering with large language models. — europe_pmc | 2025 — https://pubmed.ncbi.nlm.nih.gov/41367595/